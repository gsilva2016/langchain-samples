{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4152b4-244e-4ca1-8c64-94977a816a81",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This project demonstrates identification of food items from a food tray using open-source zero shot object detection models without finetuning, open-source embeddings models without finetuning and FAISS as the vector database. All models run locally on an Intel Core platform.\n",
    "\n",
    "Core pipeline consists of:\n",
    "1. Metadata and Image extraction from menu PDF document\n",
    "2. FastSAM for zero shot object detection\n",
    "3. Custom filter function to reduce false positives in FastSAM\n",
    "4. OpenVINO for optimized model inference on Intel platforms\n",
    "5. CLIP model for main embeddings\n",
    "6. Custom Augmentation function to increase embeddings due to less data\n",
    "7. Image identification using CLIP and FAISS (open source models without finetuning)\n",
    "8. Synthesis of vector DB retrieved data using an LVM (MiniCPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be90a0-0010-439b-9d24-cb6dd1df2405",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b79fb-68db-4f52-a01d-0d75ab9c5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q faiss-cpu Pillow torch torchvision transformers supervision --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q ultralytics pdfplumber\n",
    "%pip install -q git+https://github.com/openai/CLIP.git\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q openvino openvino-genai openvino-tokenizers nncf\n",
    "%pip install -q timm sentencepiece peft\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d902d-78a6-4a96-b3e7-74e30af1e41b",
   "metadata": {},
   "source": [
    "### Extract metadata, images from PDF\n",
    "\n",
    "This section extracts all the images from Product List PDF file into individual images. These are used an main embeddings for the image search. All associated metadata is also extracted to create a `product_data.json` file which is needed to map search results with its associated product info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb834c36-4a8e-48f4-b85a-594c95f3a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "data = {}\n",
    "\n",
    "pdf = pdfplumber.open(\"product_list.pdf\")\n",
    "\n",
    "if not os.path.exists(\"extracted\"):\n",
    "    os.makedirs(\"extracted\")\n",
    "\n",
    "for i, page in enumerate(pdf.pages):\n",
    "    if i == 0:\n",
    "        continue\n",
    "        \n",
    "    item_key = i - 1\n",
    "    data[item_key] = {\"metadata\": {\"product_names\": [], \"product_codes\": []}}\n",
    "    table = page.extract_table()\n",
    "\n",
    "    for j, image in enumerate(page.images):\n",
    "        image_data = image[\"stream\"].get_data()\n",
    "        image_path = f\"extracted/page_{i+1}_image-{i-1}.jpg\"\n",
    "        with open(image_path, \"wb\") as handle:\n",
    "            handle.write(image_data)\n",
    "        data[item_key][\"image_path\"] = image_path\n",
    "\n",
    "    if table is not None:\n",
    "        for row in table:\n",
    "            if row and item_key != 42:\n",
    "                data[item_key][\"metadata\"][\"product_codes\"].append(row[0])\n",
    "                data[item_key][\"metadata\"][\"product_names\"].append(row[1])\n",
    "            if row and item_key == 42:\n",
    "                data[item_key][\"metadata\"][\"product_codes\"].append(row[1])\n",
    "                data[item_key][\"metadata\"][\"product_names\"].append(row[2])\n",
    "\n",
    "# print(data)\n",
    "\n",
    "with open(\"product_data.json\", \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(data, handle, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a057748-c35a-4bd4-a24c-f615ab4f36ee",
   "metadata": {},
   "source": [
    "### FastSAM for ROI\n",
    "\n",
    "We use Fast SAM for zero shot object detection since it combines the benefits of YOLO and SAM. Model is converted to openVINO IR reperesentation so that it can run efficiently on an Intel GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21802649-e18d-4c23-8a4d-4ba1fb9a7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "\n",
    "import openvino as ov\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ultralytics import FastSAM\n",
    "\n",
    "model_name = \"FastSAM-x\"\n",
    "model = FastSAM(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24133a3-8ba1-497d-af56-b1e632ee916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def filter_boxes(all_boxes, shape, max_area_thr_percent=35, min_area_thr_percent=1.5, min_wh_thr_percent=9, max_wh_thr_percent=80):\n",
    "    \"\"\"\n",
    "    This function filters bounding boxes from FastSAM's output. There are multiple extraneous boxes which interefe duing the image search. The threshold\n",
    "    parameters can be changed as per the dataset. \n",
    "    \"\"\"\n",
    "    bboxes = np.array(all_boxes.data)\n",
    "    x1, y1, x2, y2, conf, _ = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3], bboxes[:, 4], bboxes[:, 5]\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    \n",
    "    area = w * h\n",
    "    #print(w, h, area)\n",
    "    minimum = (min_area_thr_percent / 100) * (shape[0] * shape[1])\n",
    "    maximum = (max_area_thr_percent / 100) * (shape[0] * shape[1]) \n",
    "    \n",
    "    valid = (area > minimum) & \\\n",
    "            (area < maximum) & (h < (max_wh_thr_percent / 100) * shape[0]) & (w < (max_wh_thr_percent / 100) * shape[1]) & (w > (min_wh_thr_percent / 100) * shape[1]) & (h > (min_wh_thr_percent / 100) * shape[0])\n",
    "    all_boxes.data = bboxes[valid]\n",
    "    \n",
    "    return all_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c4860-6942-46c1-96e0-ee3e329531d3",
   "metadata": {},
   "source": [
    "Run sample image using FastSAM to initialize model. Notice the time it takes for inference without openVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35faeaf5-eea3-45df-a1cd-8891e529242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "image_uri = Path(\"img/image_20250224_135438.jpg\")\n",
    "\n",
    "results = model(image_uri, device=\"cpu\", conf=0.65, iou=0.3)\n",
    "shape = results[0].orig_shape\n",
    "bboxes = results[0].boxes\n",
    "\n",
    "print(f\"Total num of boxes: {len(bboxes)} with model shape {shape}\")\n",
    "filtered_boxes = filter_boxes(bboxes, shape)\n",
    "\n",
    "print(f\"Total num of boxes after filter: {len(filtered_boxes)}\")\n",
    "results[0].update(boxes=filtered_boxes.data)\n",
    "\n",
    "Image.fromarray(results[0].plot(masks=False)[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85481ab-995a-4784-948c-ed0d5c27e75a",
   "metadata": {},
   "source": [
    "### OpenVINO format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8740fe5-6e84-4bdc-820f-a3212b6d4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Reference: https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/fast-segment-anything/fast-segment-anything.ipynb\n",
    "\n",
    "device = \"GPU\"\n",
    "ov_model_path = Path(f\"{model_name}_openvino_model/{model_name}.xml\")\n",
    "if not ov_model_path.exists():\n",
    "    ov_model = model.export(format=\"openvino\", dynamic=False, half=False)\n",
    "\n",
    "class OVWrapper:\n",
    "    def __init__(self, ov_model, device=\"CPU\", stride=32, ov_config=None) -> None:\n",
    "        ov_config = ov_config or {}\n",
    "        self.model = core.compile_model(ov_model, device, ov_config)\n",
    "\n",
    "        self.stride = stride\n",
    "        self.pt = False\n",
    "        self.fp16 = False\n",
    "        self.names = {0: \"object\"}\n",
    "\n",
    "    def __call__(self, im, **_):\n",
    "        result = self.model(im)\n",
    "        return torch.from_numpy(result[0]), torch.from_numpy(result[1])\n",
    "\n",
    "ov_config = {}\n",
    "core = ov.Core()\n",
    "if \"GPU\" in device or (\"AUTO\" in device and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302f5d6-e389-469c-a184-ff02d750adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_path = Path(f\"{model_name}_openvino_model/{model_name}.xml\")\n",
    "print(device)\n",
    "wrapped_model = OVWrapper(\n",
    "    ov_model_path,\n",
    "    device=device,\n",
    "    stride=model.predictor.model.stride,\n",
    "    ov_config=ov_config,\n",
    ")\n",
    "model.predictor.model = wrapped_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f683b-f663-4640-9ba2-77b3ac9bfe4c",
   "metadata": {},
   "source": [
    "Now run the input image on the openVINO optimized model to see the inference speed difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d55d5d-31b6-457f-93c6-a990228b9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_uri = Path(\"img/image_20250224_135438.jpg\")\n",
    "print(device)\n",
    "\n",
    "ov_results = model(image_uri, device=device, conf=0.65, iou=0.3)\n",
    "\n",
    "shape = ov_results[0].orig_shape\n",
    "bboxes = ov_results[0].boxes\n",
    "\n",
    "print(f\"Total num of boxes: {len(bboxes)} with model shape {shape}\")\n",
    "filtered_boxes = filter_boxes(bboxes, shape)\n",
    "\n",
    "print(f\"Total num of boxes after filter: {len(filtered_boxes)}\")\n",
    "ov_results[0].update(boxes=filtered_boxes.data)\n",
    "\n",
    "Image.fromarray(ov_results[0].plot(masks=False)[..., ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6755df7-40b6-44ff-a319-9cadf1156d9e",
   "metadata": {},
   "source": [
    "### Run OD on all sample inference images\n",
    "\n",
    "The confidence threshold, IoU threshold can be configured as per dataset requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c33ae-572b-4a99-9280-ee07cdadda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "img_folder = \"img\"\n",
    "ref_paths = [os.path.join(img_folder, f) for f in os.listdir(img_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "all_res = []\n",
    "for img_path in ref_paths:\n",
    "    ov_results = model(img_path, device=device, conf=0.65, iou=0.3)\n",
    "    \n",
    "    shape = ov_results[0].orig_shape\n",
    "    bboxes = ov_results[0].boxes\n",
    "    \n",
    "    print(f\"Total num of boxes: {len(bboxes)} with model shape {shape}\")\n",
    "    filtered_boxes = filter_boxes(bboxes, shape)\n",
    "    print(f\"Total num of boxes after filter: {len(filtered_boxes)}\\n\")\n",
    "    \n",
    "    ov_results[0].update(boxes=torch.from_numpy(filtered_boxes.data))\n",
    "    all_res.append(ov_results)\n",
    "    \n",
    "    #break\n",
    "\n",
    "num_images = len(all_res)\n",
    "cols = 3  \n",
    "rows = math.ceil(num_images/cols)\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, r in enumerate(all_res):\n",
    "        im_bgr = r[0].plot(masks=False)  \n",
    "        im_rgb = Image.fromarray(im_bgr[..., ::-1])  \n",
    "\n",
    "        ax[i].imshow(im_rgb)  \n",
    "        ax[i].set_title(f\"Image {i+1}-{r[0][0].path.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4f404-29c4-4cba-8753-0a014bfef147",
   "metadata": {},
   "source": [
    "### Create CLIP embeddings and store in FAISS\n",
    "\n",
    "We use a combination of CLIP 32 and FAISS the image search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae452e-2066-42f7-bc32-4dae18f90b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_name = \"cropped\"\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "for i, each in enumerate(all_res):\n",
    "    #print(each)\n",
    "    for j, result in enumerate(each):\n",
    "        result.save_crop(save_dir=f\"{dir_name}/{each[0].path.split('.')[0].split('/')[-1]}\", file_name=f\"detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5e74d-6157-4c15-9426-bb0c173bee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import json\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977737c-5638-40e4-be67-fc75e8377526",
   "metadata": {},
   "source": [
    "### Data Augmentations\n",
    "\n",
    "Since the number of main embeddings are few in number, we perform augmentations to increase the number of main embeddings. \n",
    "This gives us better search results. The search results are directly proportional to the number of main embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00d382-2ace-4dc9-a75a-70f3da7c80b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "image_folder = \"extracted\"\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith((\".png\", \".jpg\"))]\n",
    "count = len(image_paths)\n",
    "\n",
    "for i, path in enumerate(sorted(image_paths, key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]))):\n",
    "    try:\n",
    "        image = Image.open(path)\n",
    "    except IOError:\n",
    "        print(\"Error loading:\", path)\n",
    "        continue\n",
    "        \n",
    "    name = Path(path).stem.split(\"-\")[0]\n",
    "    rotated_90 = image.rotate(90, expand=True)\n",
    "    rotated_90.save(f\"{image_folder}/{i}_{name}-{i + count}.jpg\")\n",
    "\n",
    "    count += 1\n",
    "    rotated_270 = image.rotate(-90, expand=True)\n",
    "    rotated_270.save(f\"{image_folder}/{i}_{name}-{i + count}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191e028-81ad-41d2-af6e-82d9ae4239bb",
   "metadata": {},
   "source": [
    "### Load CLIP and populate the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca1caf-c0bc-4a26-9b35-a0a7824fab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcb51d-c3c7-4b64-8b90-9a691502ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image: Image) -> np.ndarray:\n",
    "    image = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.encode_image(image)\n",
    "    return embedding.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328429e-9fd3-4bfd-867a-7c30ac6be132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clip_index = faiss.IndexFlatL2(512)\n",
    "\n",
    "image_folder = \"extracted\"\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith((\".png\", \".jpg\"))]\n",
    "file_names = []\n",
    "\n",
    "for i, path in enumerate(sorted(image_paths, key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]))):\n",
    "    try:\n",
    "        frame = Image.open(path)\n",
    "    except IOError:\n",
    "        print(\"Error loading:\", path)\n",
    "        continue\n",
    "\n",
    "    embedding = get_image_embedding(frame)\n",
    "    clip_index.add(np.array([embedding]).astype(np.float32))\n",
    "    #file_names.append(path)\n",
    "\n",
    "faiss.write_index(clip_index, \"clip_index.bin\")\n",
    "print(\"FAISS Index saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ccb91-abd7-43a4-b594-4940c9c2cefd",
   "metadata": {},
   "source": [
    "Update the metadata JSON file with the new augmented images and its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1cb6c-2d61-4e31-b82c-f3c02291dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"product_data.json\", \"r\") as handle:\n",
    "    product_data = json.load(handle)\n",
    "\n",
    "image_folder = \"extracted\"\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith((\".png\", \".jpg\"))]\n",
    "\n",
    "new_data = product_data.copy()\n",
    "for i, path in enumerate(sorted(image_paths, key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]))):\n",
    "    if new_data.get(str(i)) is None:\n",
    "        ref_path = path.split(\"_\")[0].split(\"/\")[1]\n",
    "        new_data[str(i)] = product_data[ref_path]\n",
    "    print(i, new_data[str(i)][\"image_path\"])\n",
    "\n",
    "with open(\"product_data.json\", \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(new_data, handle, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d677c5e-2cbb-494c-bd29-77f5fa307002",
   "metadata": {},
   "source": [
    "### Inference and Image Search Test with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b776e-9079-4aa2-9272-2cc177653097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import faiss\n",
    "\n",
    "\n",
    "query_folder = \"cropped\"\n",
    "img_folder = \"img\"\n",
    "query_paths = [os.path.join(query_folder, f) for f in os.listdir(query_folder) if f.endswith((\".png\", \".jpg\"))]\n",
    "\n",
    "image_folder = \"extracted\"\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith((\".png\", \".jpg\"))]\n",
    "sorted_imgs = sorted(image_paths, key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]))\n",
    "\n",
    "clip_index = faiss.read_index(\"clip_index.bin\")\n",
    "\n",
    "with open(\"product_data.json\", \"r\") as f:\n",
    "    product_data = json.load(f)\n",
    "    \n",
    "max_distance_thres = 75\n",
    "final_results = {}\n",
    "\n",
    "for sub in sorted(os.listdir(query_folder)):\n",
    "    if sub.startswith(\"image_\"):\n",
    "        subf = pathlib.Path(os.path.join(query_folder, sub))\n",
    "        print(f\"Input image: img/{sub}.jpg\")\n",
    "        final_results[f\"img/{sub}.jpg\"] = []\n",
    "        \n",
    "        input_img = cv2.imread(f\"img/{sub}.jpg\")\n",
    "        sv.plot_image(input_img, (5, 5))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Matched items and top item's metadata:\")\n",
    "        \n",
    "        # ignore ipynb project entries\n",
    "        query_paths = [str(f) for f in subf.rglob(\"*.jpg\") if \"ipynb\" not in str(f)]\n",
    "        for i, query_image in enumerate(query_paths):\n",
    "            image = Image.open(query_image)\n",
    "            query_embedding = get_image_embedding(image)\n",
    "            dist, ind = clip_index.search(np.array([query_embedding]).astype(np.float32), 4)\n",
    "            #print(dist[0])\n",
    "            \n",
    "            filtered_images = []\n",
    "            results = []\n",
    "\n",
    "            for idx, distance in zip(ind[0], dist[0]):\n",
    "                if distance < max_distance_thres:\n",
    "                    filtered_images.append(cv2.imread(sorted_imgs[idx]))\n",
    "                    if str(idx) in product_data:\n",
    "                        results.append(product_data[str(idx)])\n",
    "            if results:\n",
    "                print(results[0])\n",
    "                final_results[f\"img/{sub}.jpg\"].append(results[0])\n",
    "                     \n",
    "            query_image = cv2.imread(query_image)\n",
    "            sv.plot_image(query_image, (2, 2))\n",
    "        \n",
    "            if filtered_images:\n",
    "                sv.plot_images_grid(filtered_images, grid_size=(3,4), size=(6,6))\n",
    "            else:\n",
    "                print(\"No matches found\")\n",
    "        #break\n",
    "\n",
    "with open(\"result.json\", \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(final_results, handle, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234da2db-c2d2-462b-9327-c246d152b7d8",
   "metadata": {},
   "source": [
    "### LVM Synthesis using MiniCPM V2.6\n",
    "\n",
    "Here we use MiniCPM to synthesize the vector DB results. We use an openVINO optimized version for best performance on an Intel GPU. This model can be switched to another LVM of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3eb4c4-7883-4cbf-b17c-7f788d731671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export HF_TOKEN=\"<insert HF token here>\"\n",
    "#!optimum-cli export openvino -m openbmb/MiniCPM-o-2_6 --trust-remote-code --weight-format int4 minicpm_int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcfe13-e8fa-4a9e-887b-6951e638f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_genai as ov_genai\n",
    "from pathlib import Path\n",
    "\n",
    "device = \"GPU\"\n",
    "model_dir = Path(\"<path to minicpm OV dir>\")\n",
    "\n",
    "ov_model = ov_genai.VLMPipeline(model_dir, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e965c2-eff9-4b76-9e42-df0ec7afe182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "\n",
    "config = ov_genai.GenerationConfig()\n",
    "config.max_new_tokens = 500\n",
    "\n",
    "def load_image(image_file):\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image_data = np.array(image.getdata()).reshape(1, image.size[1], image.size[0], 3).astype(np.byte)\n",
    "    return image, ov.Tensor(image_data)\n",
    "\n",
    "def streamer(subword: str) -> bool:\n",
    "    print(subword, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8915ea-de74-4f01-8533-468e8c54d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "with open(\"result.json\", \"r\") as handle:\n",
    "    final_results = json.load(handle)\n",
    "\n",
    "question = \"\"\"\n",
    "As a food analyzer, please analyze this image. Please use the context for reference and provide a summary with product names and product codes. Only use the items listed in the context. Do not make up new items.\n",
    "        \n",
    "Context:\n",
    "\"\"\"\n",
    "for image_path, entries in final_results.items():    \n",
    "    image, image_tensor = load_image(image_path)\n",
    "    display(image)\n",
    "\n",
    "    for i, entry in enumerate(entries):\n",
    "        question += f\"\"\"\n",
    "        Item {i+1}: {entry}\"\"\"\n",
    "    break  \n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627c5e6-5fc6-46f0-840d-516c5c83ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ov_model.start_chat()\n",
    "output = ov_model.generate(question, image=image_tensor, generation_config=config, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611f445-f9d4-48af-8e8f-9cfb16dc1660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skk_new_mcd",
   "language": "python",
   "name": "skk_new_mcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
